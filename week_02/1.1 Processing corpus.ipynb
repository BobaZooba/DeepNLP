{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов - 1157366, слов - 16028874'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Текстов - {}, слов - {}'.format(len(corpus), sum([len(sample) for sample in corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    \n",
    "    word_data = morph.parse(word)[0]\n",
    "    \n",
    "    return word_data.normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соберем словарь лем\n",
    "В нашем корпусе 16028874 слов. Лемматизировать весь корпус будет очень долго. Давайте лучше соберем словарь уникальных слов и будет лемматизировать только уникальные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:38<00:00, 30225.70it/s]\n"
     ]
    }
   ],
   "source": [
    "tok2lemma = {}\n",
    "\n",
    "for text in tqdm(corpus):\n",
    "    for tok in text:\n",
    "        if tok not in tok2lemma:\n",
    "            tok2lemma[tok] = get_lemma(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'уехать'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ключ - уникальное слово в нашем корпусе, значение - его лемма\n",
    "tok2lemma['уехали']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193435"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok2lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена слов леммами\n",
    "Так как теперь к каждому слову из нашего корпуса мы знаем лемму, то давайте каждое слово заменим на его лемму и уберем стоп слова.\n",
    "Это работает гораздо(!) быстрее, чем если бы мы в корпусе для каждого слова каждый раз рассчитывали лемму (с помощью пайморфи), \n",
    "потому что теперь нам надо вызвать пайморфи 193435 раз вместо 16028874."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:24<00:00, 46555.20it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmas_corpus = [[tok2lemma[tok] for tok in text if tok not in stopwords and tok]\n",
    "                 for text in tqdm(corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем частотный словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:02<00:00, 395416.91it/s]\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "\n",
    "for text in tqdm(lemmas_corpus):\n",
    "    for tok in text:\n",
    "        if tok in freq:\n",
    "            freq[tok] += 1\n",
    "        else:\n",
    "            freq[tok] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame(data={'word': list(freq.keys()), 'n_entries': list(freq.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.sort_values(by=['n_entries'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>##число</td>\n",
       "      <td>413016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>банка</td>\n",
       "      <td>197884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>карта</td>\n",
       "      <td>156216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>банк</td>\n",
       "      <td>135943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>кредит</td>\n",
       "      <td>86865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  n_entries\n",
       "21   ##число     413016\n",
       "3      банка     197884\n",
       "47     карта     156216\n",
       "25      банк     135943\n",
       "134   кредит      86865"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77002, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# уникальных слов в словаре\n",
    "freq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45645</th>\n",
       "      <td>пролом</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45647</th>\n",
       "      <td>запачкать</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45648</th>\n",
       "      <td>гих</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>поcтоянно</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77001</th>\n",
       "      <td>ситуевина</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  n_entries\n",
       "45645     пролом          1\n",
       "45647  запачкать          1\n",
       "45648        гих          1\n",
       "45650  поcтоянно          1\n",
       "77001  ситуевина          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = freq_df.n_entries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена редких слов\n",
    "В нашем корпусе осталось много слов, которые встречаются очень редко. Давайте мы редкие слова заменим на специальный токе UNK - unknown. Так мы разительно сократим размер нашего словаря слов с незначительной потерей информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля слов, которые мы заменим на UNK:\n",
      "Порог отсечения - 5, доля UNK - 0.76 %, слов в слове - 23389, удалили - 53613 слов\n",
      "Порог отсечения - 10, доля UNK - 1.18 %, слов в слове - 16851, удалили - 60151 слов\n",
      "Порог отсечения - 15, доля UNK - 1.51 %, слов в слове - 13861, удалили - 63141 слов\n",
      "Порог отсечения - 20, доля UNK - 1.82 %, слов в слове - 12010, удалили - 64992 слов\n",
      "Порог отсечения - 25, доля UNK - 2.07 %, слов в слове - 10798, удалили - 66204 слов\n",
      "Порог отсечения - 30, доля UNK - 2.31 %, слов в слове - 9882, удалили - 67120 слов\n",
      "Порог отсечения - 35, доля UNK - 2.53 %, слов в слове - 9170, удалили - 67832 слов\n"
     ]
    }
   ],
   "source": [
    "print('Доля слов, которые мы заменим на UNK:')\n",
    "\n",
    "for threshold in np.arange(5, 36, 5):\n",
    "    \n",
    "    sub_df = freq_df[freq_df.n_entries < threshold]\n",
    "    \n",
    "    unk_freq = sub_df['n_entries'].sum() * 100 / n_words\n",
    "    \n",
    "    print('Порог отсечения - {}, доля UNK - {:.2f} %, слов в слове - {}, удалили - {} слов'.format(\n",
    "        threshold, unk_freq, freq_df.shape[0] - sub_df.shape[0], sub_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кажется, что оптимально, но обычно берут меньше\n",
    "threshold = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = freq_df[freq_df.n_entries >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(vocab.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13861"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мы сократили наш словарь в 5.56 раз с потерей 1.51 % всех слов'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мы сократили наш словарь в {:.2f} раз с потерей 1.51 % всех слов'.format(freq_df.shape[0] / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_words(word):\n",
    "    \n",
    "    if word in words:\n",
    "        return word\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:04<00:00, 265597.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# заменим слово токеном UNK, если его нет в нашем новом словаре\n",
    "processed_corpus = [[get_correct_words(tok) for tok in text] for text in tqdm(lemmas_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_unks(tokens):\n",
    "    \n",
    "    output_tokens = []\n",
    "    \n",
    "    for tok in tokens:\n",
    "        \n",
    "        if tok == 'UNK' and output_tokens and output_tokens[-1] == 'UNK':\n",
    "            continue\n",
    "            \n",
    "        output_tokens.append(tok)\n",
    "            \n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'думать далее милый барышня UNK UNK тинькоф звонить неделя'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать',\n",
       " 'далее',\n",
       " 'милый',\n",
       " 'барышня',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'тинькоф',\n",
       " 'звонить',\n",
       " 'неделя']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать', 'далее', 'милый', 'барышня', 'UNK', 'тинькоф', 'звонить', 'неделя']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_duplicate_unks(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:03<00:00, 348464.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# дедублируем подряд идущие унки (оставим только один)\n",
    "processed_corpus = [drop_duplicate_unks(sample) for sample in tqdm(processed_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов с унками - 11.28 %'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_unk = [text for text in processed_corpus if 'UNK' in text]\n",
    "'Текстов с унками - {:.2f} %'.format(len(texts_with_unk) * 100 / len(processed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нужный международный стандарт UNK знать такой бред этот momentum\n",
      "сегодня посетить филиал банка UNK хотеть узнать кредитный карта условие\n",
      "стать хороший UNK ваш софт UNK рынок весь счастие\n",
      "норматив рассмотрение заявка ##число день комиссия вообще UNK четверг\n",
      "обслуживать шаг ва UNK приятно удивить отношение сотрудник банка клиент\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на тексты с унками\n",
    "for text in random.sample(texts_with_unk, k=5):\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выглядит не так плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выберем подвыборку данных\n",
    "Чтобы быстрее выучить word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = processed_corpus[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed_corpus.json', 'w') as f:\n",
    "    json.dump(sub_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
