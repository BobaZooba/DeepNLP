{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from data import Downloader, Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем файл с эмбеддингами для английского языка\n",
    "Они нам понадобятся чуть позже.  \n",
    "Для других языков можете найти здесь: https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-05 06:56:55--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Распознаётся dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)… 172.67.9.4, 104.22.74.142, 104.22.75.142\n",
      "Подключение к dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 681808098 (650M) [application/zip]\n",
      "Сохранение в: «wiki-news-300d-1M.vec.zip»\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650,22M  9,35MB/s    за 71s     \n",
      "\n",
      "2020-10-05 06:58:08 (9,13 MB/s) - «wiki-news-300d-1M.vec.zip» сохранён [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# раскомментируйте и скачайте\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# путь к данным\n",
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Читалка данные\n",
    "Не стоит вдаваться в подробности, просто эта штука скачивает данные, затем парсит и делает из них три датасета:\n",
    "- тренировочный\n",
    "- валидационный\n",
    "- неразмеченный\n",
    "\n",
    "Неразмеченные данные необазятельны, но могут вам понадобиться, например, для языковой модели или улучшения эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = Downloader(data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "single: 100%|██████████| 21/21 [02:18<00:00,  6.60s/it]\n",
      "multiple: 100%|██████████| 17/17 [03:46<00:00, 13.32s/it]\n"
     ]
    }
   ],
   "source": [
    "downloader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled, train, valid = parser.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача\n",
    "Классифицировать поле question в одну из категорий в поле category.  \n",
    "Это данные с сервиса Amazon QA, то есть такой сервис, на котором можно задать вопрос и получить ответ от других пользователей.\n",
    "\n",
    "Идея задачи такая: давайте поможем клиенту определить в какую категорию выложить его вопрос, чтобы быстрее получить максимально релевантный ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевод класса в индекс\n",
    "Мы сделаем некоторый маппер, который текст класса переводит в конкретный уникальный индекс. Нам это понадобиться, потому что наша \n",
    "модель работает не напрямую с классом, а с его индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим, что в трейне и валидации одинаковые категории\n",
    "set(train.category.unique().tolist()) == set(valid.category.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = set(train.category.unique().tolist() + valid.category.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2index = {category: index for index, category in enumerate(unique_categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train.category.map(category2index)\n",
    "valid['target'] = valid.category.map(category2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataset, DataLoader\n",
    "\n",
    "Очень важная абстракция для торча.\n",
    "Мы всегда будем ее использовать, чтобы работать с данными.\n",
    "\n",
    "Dataset - класс, от которого нужно наследоваться, чтобы написать свой обработчик данных. Внутри него нужно реализовать два метода, \n",
    "о которых будет чуть ниже. То есть в данном классе вы описывает как нужно преобразовать ваши данные в торчовый формат. Перевести тексты \n",
    "в индексы слов и тд.\n",
    "\n",
    "DataLoader - класс, который будет за вас семплировать данные батчами. Это итератор, поэтому формат работы с ним примерно такой:\n",
    "```python\n",
    "for batch in data_loader:\n",
    "    ...\n",
    "```\n",
    "То есть на каждой итерации отдается по одному батчу данных. Итерирование заканчивается, когда вы пройдете все батчи.\n",
    "\n",
    "Зачем нужны эти абстракции? Чтобы упростить и унифицировать работу с данными.\n",
    "Вообще вы можете реализовать что-то свое, но это упрощение данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# игрушечный датасет\n",
    "# 121535 примера, 4 фичи, 3 класса\n",
    "some_data_x = np.random.rand(121535, 4)\n",
    "some_data_y = np.random.randint(3, size=(121535,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просто рандомные цифры\n",
    "some_data_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# и классы\n",
    "some_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример надобности\n",
    "Для обучения модели вам нужно подавать в нее батчи данных. Как бы могли это реализовать, если бы у нас не было Dataset и DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "for i_batch in range(math.ceil(some_data_x.shape[0] / batch_size)):\n",
    "    \n",
    "    x_batch = some_data_x[i_batch * batch_size:(i_batch + 1) * batch_size]\n",
    "    y_batch = some_data_y[i_batch * batch_size:(i_batch + 1) * batch_size]\n",
    "    \n",
    "    x_batch = torch.tensor(x_batch)\n",
    "    y_batch = torch.tensor(y_batch)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это достаточно простой пример. Мы смогли справиться сами, но почти всегда обработка данных для подачи ее в модель делается сложнее. \n",
    "И некоторые вещи часто нужны более одного раза, например, если мы хотим каждую эпоху шафлить наши данные, чтобы получать разные батчи.\n",
    "Мы сможем это сделать, но для этого нам придется тащить с собой некоторый код из проекта в проект. К тому же совместная разработка или \n",
    "просто чтение чужого кода сильно упрощается, когда вы используете унифицированные форматы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перейдем к Dataset\n",
    "И обернем наши данные в этот обработчик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_x, data_y):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        # нужно обязательно определить эту функцию\n",
    "        # должна возвращать размер датасета\n",
    "        # нужен для DataLoader, чтобы семплировать батчи\n",
    "        \n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # еще нужно определить этот метод\n",
    "        # то есть как мы будем доставать наши данные по индексу\n",
    "        \n",
    "        return self.data_x[idx], self.data_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataset = ToyDataset(some_data_x, some_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataset[5], some_dataset[467]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кажется, что смысла в этом нет, но это самый простой пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "В него мы можем задать некоторые параметры, например, батч сайз и нужно ли шафлить каждый новый проход по данным эти самые данные, \n",
    "чтобы получать разные батчи, то есть по разному компоновать эти батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_loader = DataLoader(some_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in some_loader:\n",
    "    break\n",
    "    \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in some_loader:\n",
    "    pass\n",
    "\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# почему 15?\n",
    "# потому что количество наших данных нацело не делится на 16\n",
    "# и поэтому последний батч меньше 16-ти\n",
    "len(some_dataset) % 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Усложним обработчик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_x, data_y):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        # нужно обязательно определить эту функцию\n",
    "        # должна возвращать размер датасета\n",
    "        # нужен для DataLoader, чтобы семплировать батчи\n",
    "        \n",
    "        return len(self.data_x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def pow_features(x, n=2):\n",
    "        \n",
    "        return x ** n\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_features(x):\n",
    "        \n",
    "        return np.log(x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # еще нужно определить этот метод\n",
    "        # то есть как мы будем доставать наши данные по индексу\n",
    "        \n",
    "        x = self.data_x[idx]\n",
    "        \n",
    "        # внутри датасета мы можем делать все что угодно с нашими данными\n",
    "        # например выше определим функции, которые добавляют степенные фичи\n",
    "        x_p_2 = self.pow_features(x, n=2)\n",
    "        x_p_3 = self.pow_features(x, n=3)\n",
    "        # и еще возьмем логарифмические фичи\n",
    "        x_log = self.log_features(x)\n",
    "        \n",
    "        # сконкатенируем наши фичи\n",
    "        x = np.concatenate([x, x_p_2, x_p_3, x_log])\n",
    "        \n",
    "        y = self.data_y[idx]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = ToyDataset(some_data_x, some_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_loader = DataLoader(dataset=toy_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in toy_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заметим, что мы сразу получаем торчовый формат данных, который получился из автоматического преобразования из numpy\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем небольшую модель и посчитаем лосс\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(16, 8),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(8, 4),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(4, 3))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    prediction = model(x.float())\n",
    "\n",
    "    loss = criterion(prediction, y)\n",
    "    \n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем датасет для наших текстовых данных\n",
    "Будем отдавать строку и таргет по индексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, targets):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.texts[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        return text, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовим данные\n",
    "train_x = list(train.question)\n",
    "train_y = list(train.target)\n",
    "\n",
    "valid_x = list(valid.question)\n",
    "valid_y = list(valid.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(texts=list(train.question), targets=list(train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# семплируем данные\n",
    "text, target = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Смысл обработчика\n",
    "Состоит в том, что нам нужно преобразовать наши данные в формат, который мы уже сможем передать в модель.\n",
    "Сейчас у нас строки, а торч ничего не знает про строки, ему нужны тензоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем эмбеддинги\n",
    "Чтобы работать с текстовыми данными мы можем разбить наши строки на слова, а слова перевести в вектора. Откуда нам взять эти вектора?\n",
    "Мы говорили про такой метод как word2vec и в начале этой тетрадки загружали файл с этими самыми векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(zip_path, filename, pad_token='PAD', max_words=100_000, verbose=True):\n",
    "    \n",
    "    vocab = dict()\n",
    "    embeddings = list()\n",
    "\n",
    "    with zipfile.ZipFile(zip_path) as zipped_file:\n",
    "        with zipped_file.open(filename) as file_object:\n",
    "\n",
    "            vocab_size, embedding_dim = file_object.readline().decode('utf-8').strip().split()\n",
    "\n",
    "            vocab_size = int(vocab_size)\n",
    "            embedding_dim = int(embedding_dim)\n",
    "            \n",
    "            # в файле 1 000 000 слов с векторами, давайте ограничим для простоты этот словарь\n",
    "            max_words = vocab_size if max_words <= 0 else max_words\n",
    "            \n",
    "            # добавим пад токен и эмбеддинг в нашу матрицу эмбеддингов и словарь\n",
    "            vocab[pad_token] = len(vocab)\n",
    "            embeddings.append(np.zeros(embedding_dim))\n",
    "\n",
    "            progress_bar = tqdm(total=max_words, disable=not verbose)\n",
    "\n",
    "            for line in file_object:\n",
    "                parts = line.decode('utf-8').strip().split()\n",
    "\n",
    "                token = ' '.join(parts[:-embedding_dim]).lower()\n",
    "                \n",
    "                if token in vocab:\n",
    "                    continue\n",
    "                \n",
    "                word_vector = np.array(list(map(float, parts[-embedding_dim:])))\n",
    "\n",
    "                vocab[token] = len(vocab)\n",
    "                embeddings.append(word_vector)\n",
    "\n",
    "                progress_bar.update()\n",
    "                \n",
    "                if len(vocab) == max_words:\n",
    "                    break\n",
    "\n",
    "            progress_bar.close()\n",
    "\n",
    "    embeddings = np.stack(embeddings)\n",
    "    \n",
    "    return vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab, embeddings = load_embeddings('./wiki-news-300d-1M.vec.zip', 'wiki-news-300d-1M.vec', max_words=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на ближайших соседей слова по эмбеддингам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2token = {index: token for token, index in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_norms = np.linalg.norm(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_neighbors(word, embeddings, emb_norms, vocab, index2token, k=5):\n",
    "    \n",
    "    if word not in vocab:\n",
    "        print('Not in vocab')\n",
    "        return\n",
    "    \n",
    "    word_index = vocab[word]\n",
    "\n",
    "    word_vector = embeddings[word_index]\n",
    "    word_vector = np.expand_dims(word_vector, 0)\n",
    "\n",
    "    scores = (word_vector @ embeddings.T)[0]\n",
    "    \n",
    "    # переводим в косинусы, поделив на нормы векторов\n",
    "    # эпсилон 1e-6 для того, чтобы не делить на 0\n",
    "    scores = scores / (emb_norms + 1e-6) / emb_norms[word_index]\n",
    "    \n",
    "    # 1:k+1 потому что первый вариант это само слово\n",
    "    for idx in scores.argsort()[::-1][1:k+1]:\n",
    "        print(f'Слово {index2token[idx]} близко на {scores[idx]:.2f} к слову {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors('anna', embeddings, emb_norms, vocab, index2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors('mom', embeddings, emb_norms, vocab, index2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors('have', embeddings, emb_norms, vocab, index2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors('money', embeddings, emb_norms, vocab, index2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors('music', embeddings, emb_norms, vocab, index2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор метода токенизации\n",
    "У нас сейчас есть маппинг, что некоторому слову соответствует некоторый эмбеддинг этого слова.\n",
    "Токенизация - процесс разбиения текста на токены, то есть части этого текста.   \n",
    "Чем \"слово\" отличается от \"токена\": токен это более обобщенное понятие, то есть, например, цифра это токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# про различия подробнее можно найти, например, здесь\n",
    "# https://stackoverflow.com/questions/50240029/nltk-wordpunct-tokenize-vs-word-tokenize\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_words = 0\n",
    "unknown_words = list()\n",
    "\n",
    "for sample in tqdm(train_x):\n",
    "    # токенизация по пробелу\n",
    "    tokens = sample.split()\n",
    "    \n",
    "    for tok in tokens:\n",
    "        # проверяем есть ли токен в нашем словаре\n",
    "        if tok not in vocab:\n",
    "            unknown_words.append(tok)\n",
    "            \n",
    "        total_n_words += 1\n",
    "        \n",
    "print(f'Мы не знаем {len(unknown_words)} слов из {total_n_words} слов в датасете')\n",
    "print(f'Что составляет {len(unknown_words) * 100 / total_n_words:.2f}% датасета')\n",
    "print()\n",
    "print(f'Уникальных неизвестных слов: {len(set(unknown_words))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_words = 0\n",
    "unknown_words = list()\n",
    "\n",
    "for sample in tqdm(train_x):\n",
    "    # токенизация\n",
    "    tokens = wordpunct_tokenize(sample)\n",
    "    \n",
    "    for tok in tokens:\n",
    "        # проверяем есть ли токен в нашем словаре\n",
    "        if tok not in vocab:\n",
    "            unknown_words.append(tok)\n",
    "            \n",
    "        total_n_words += 1\n",
    "        \n",
    "print(f'Мы не знаем {len(unknown_words)} слов из {total_n_words} слов в датасете')\n",
    "print(f'Что составляет {len(unknown_words) * 100 / total_n_words:.2f}% датасета')\n",
    "print()\n",
    "print(f'Уникальных неизвестных слов: {len(set(unknown_words))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_words = 0\n",
    "unknown_words = list()\n",
    "\n",
    "for sample in tqdm(train_x):\n",
    "    # токенизация\n",
    "    tokens = word_tokenize(sample)\n",
    "    \n",
    "    for tok in tokens:\n",
    "        # проверяем есть ли токен в нашем словаре\n",
    "        if tok not in vocab:\n",
    "            unknown_words.append(tok)\n",
    "            \n",
    "        total_n_words += 1\n",
    "        \n",
    "print(f'Мы не знаем {len(unknown_words)} слов из {total_n_words} слов в датасете')\n",
    "print(f'Что составляет {len(unknown_words) * 100 / total_n_words:.2f}% датасета')\n",
    "print()\n",
    "print(f'Уникальных неизвестных слов: {len(set(unknown_words))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты\n",
    "- Скорость у word_tokenize сильно ниже, чем у wordpunct_tokenize\n",
    "- Используя word_tokenize, мы теряем примерно 1% информации из датасета по сравнению с wordpunct_tokenize\n",
    "\n",
    "### Выбор очевиден в сторону wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, targets, vocab):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def tokenization(self, text):\n",
    "        \n",
    "        tokens = wordpunct_tokenize(text)\n",
    "        \n",
    "        token_indices = [self.vocab[tok] for tok in tokens if tok in self.vocab]\n",
    "        \n",
    "        return token_indices\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.texts[index]        \n",
    "        target = self.targets[index]\n",
    "        \n",
    "        tokenized_text = self.tokenization(text)\n",
    "        \n",
    "        # переведем наши индексы токенов в торчовый тензор\n",
    "        # таргет переведется самостоятельно\n",
    "        tokenized_text = torch.tensor(tokenized_text)\n",
    "        \n",
    "        return tokenized_text, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# мы можем восстановить текст обратно по индексам слов\n",
    "[index2token[idx.item()] for idx in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### У нас остается проблема разных длин текстов\n",
    "Чтобы поместить батч текстов в один тензор нам нужны одинаковые длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## это не отработает, можете раскомментировать и проверить\n",
    "\n",
    "# x = [\n",
    "#     [1, 2, 3],\n",
    "#     [1, 2, 3, 4, 5],\n",
    "#     [1, 2, 3, 4, 5, 6, 7]\n",
    "# ]\n",
    "\n",
    "# torch.tensor(x), torch.tensor(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это сработает\n",
    "\n",
    "x = [\n",
    "    [1, 2, 3, 0, 0, 0, 0],\n",
    "    [1, 2, 3, 4, 5, 0, 0],\n",
    "    [1, 2, 3, 4, 5, 6, 7]\n",
    "]\n",
    "\n",
    "torch.tensor(x), torch.tensor(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Длина текста\n",
    "Нам нужно понять до какой длины нам падить каждый наш пример. \n",
    "Мы можем найти в наших данных максимальную длину примера в токенах и падить до этой длины, но у этого подхода есть минус:\n",
    "у нас могут быть несколько текстов с аномально большой длиной, то есть некоторые выбросы.  \n",
    "\n",
    "В таком случае нам легче ограничить длину этих текстов до определенной статистики по нашему датасет, то есть, например, 95% наших текстов\n",
    "длиной в 25 слов и нам этого достаточно. То есть мы ограничимся этой длиной, потому что почти весь датасет влезает в эту длину\n",
    "и нам не нужно будет падить до большой длины.\n",
    "\n",
    "Паддинг нужен нам для того, чтобы мы могли поместить разные примеры в один батч, но мы не хотим учитывать эти токены, то есть \n",
    "по сути это будут холостые прогоны и за счет этого компромисса, что бОльшая часть датасета не больше n слов мы можем оптимизировать \n",
    "наше обучение.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "> Почему бы нам просто не выкинуть эти длинные тексты?\n",
    "\n",
    "Дело в том, что мы хотим прийти к некоторому компромиссу между максимальной длиной и потерей информации. Если мы возьмем 95-й перцинтиль наших длин (то есть 95% наших текстов не больше n), то, выкинув остальные 5%, мы потеряем существенную часть примеров.\n",
    "С другой стороны может показаться неправильным ограничение длины и это действительно может сломать смысл примеры, но зачастую этим \n",
    "принебрегают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = [len(wordpunct_tokenize(sample)) for sample in tqdm(train_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видим большие выбросы в данных\n",
    "# 97% наших текстов не больше вот стольки токенов\n",
    "np.percentile(train_lengths, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, targets, vocab, pad_index=0, max_length=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def tokenization(self, text):\n",
    "        \n",
    "        tokens = wordpunct_tokenize(text)\n",
    "        \n",
    "        token_indices = [self.vocab[tok] for tok in tokens if tok in self.vocab]\n",
    "        \n",
    "        return token_indices\n",
    "    \n",
    "    def padding(self, tokenized_text):\n",
    "        \n",
    "        tokenized_text = tokenized_text[:self.max_length]\n",
    "        \n",
    "        tokenized_text += [self.pad_index] * (self.max_length - len(tokenized_text))\n",
    "        \n",
    "        return tokenized_text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.texts[index]        \n",
    "        target = self.targets[index]\n",
    "        \n",
    "        tokenized_text = self.tokenization(text)\n",
    "        tokenized_text = self.padding(tokenized_text)\n",
    "        \n",
    "        tokenized_text = torch.tensor(tokenized_text)\n",
    "        \n",
    "        return tokenized_text, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[index2token[idx.item()] for idx in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)\n",
    "valid_dataset = TextClassificationDataset(texts=valid_x, targets=valid_y, vocab=vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как мы можем задавать слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab), \n",
    "                               embedding_dim=embeddings.shape[-1],\n",
    "                               padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed = embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Но мы ведь прочитали нашу матрицу эмбеддингов\n",
    "Таким образом она инициализируется предобученными весами.  \n",
    "При такой инициализации по умолчанию она замораживается, внутри ```.from_pretrained(embeddings, padding_idx=0)``` есть флаг ```freeze```, который отвечает за необходимость заморозки весов. То есть эти веса в процессе обучения не будут обновляться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.tensor(embeddings).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(embeddings, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed = embedding_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного LSTM\n",
    "Ниже будет про ```batch_first=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=300, hidden_size=128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lstm, _ = lstm(x_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256 потому что это конкатенация лстмки, которая прочитала текст слева направо\n",
    "# и лстмки, которая прочитала текст справа налево\n",
    "x_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# избавились от временной размерности\n",
    "x_lstm.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем свою сеть\n",
    "В первой домашке в конце есть более подробная информация почему мы используем классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAverageNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, linear_1_size, linear_2_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings, padding_idx=0)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=embeddings.shape[-1])\n",
    "        \n",
    "        self.linear_1 = nn.Linear(in_features=embeddings.shape[-1], out_features=linear_1_size)\n",
    "        self.linear_2 = nn.Linear(in_features=linear_1_size, out_features=linear_2_size)\n",
    "        self.linear_3 = nn.Linear(in_features=linear_2_size, out_features=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # переводим индексы слов в эмбеддинги этих слов\n",
    "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        x = self.embedding_layer(x)\n",
    "        \n",
    "        # агрегируем наши эмбеддинги по размерности время\n",
    "        # (batch_size, sequence_length, embedding_dim) -> (batch_size, embedding_dim)\n",
    "        x = x.sum(dim=1)\n",
    "        \n",
    "        # делаем нормирование\n",
    "        # (batch_size, embedding_dim) -> (batch_size, embedding_dim)\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # прогоняем через первый линейный слой\n",
    "        # (batch_size, embedding_dim) -> (batch_size, linear_1_size)\n",
    "        x = self.linear_1(x)\n",
    "        \n",
    "        # применяем нелинейность\n",
    "        # (batch_size, linear_1_size) -> (batch_size, linear_1_size)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # прогоняем через второй линейный слой\n",
    "        # (batch_size, linear_1_size) -> (batch_size, linear_2_size)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        # применяем нелинейность\n",
    "        # (batch_size, linear_2_size) -> (batch_size, linear_2_size)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # переводим с помощью линейного преобразования в количество классов\n",
    "        # (batch_size, linear_2_size) -> (batch_size, n_classes)\n",
    "        x = self.linear_3(x)\n",
    "        \n",
    "        ## по идеи здесь должен был быть софтмакс\n",
    "        ## но мы будем использовать лосс nn.CrossEntropyLoss()\n",
    "        ## в его документации написано\n",
    "        ## This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.\n",
    "        ## это некоторая оптимизация, которая включает в себя сразу и софтмакс и сам negative log likelihood лосс\n",
    "        ## так как у нас в лоссе есть софтмакс, то мы не будем применять его в сетке\n",
    "        ## на этапе предсказания (а не обучения) мы будем отдельно делать софтмакс для получения распределения классов\n",
    "        ## \n",
    "        ## (batch_size, n_classes) -> (batch_size, n_classes)\n",
    "        # x = torch.softmax(x, dim=-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepAverageNetwork(embeddings=embeddings,\n",
    "                           linear_1_size=256, \n",
    "                           linear_2_size=128, \n",
    "                           n_classes=len(category2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# задайте оптимизатор\n",
    "# optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишите цикл обучения\n",
    "Что он должен в себя включать:\n",
    "1. Получение предсказаний модели\n",
    "1. Расчет функции потерь\n",
    "1. Расчет градиентов\n",
    "1. Шаг градиентного спуска\n",
    "1. Обнуление градиентов\n",
    "1. Записывание значения лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "\n",
    "# в обучении моделей у нас есть такая ситуация, что некоторые слои ведут себя по разному на этапе тренировки и предсказания\n",
    "# например, батч норм (а так же все остальные нормировки) и дропаут\n",
    "# это переводит модель в режим тренировки\n",
    "model.train()\n",
    "\n",
    "for x, y in train_loader:\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишите цикл валидации\n",
    "Что он должен в себя включать:\n",
    "1. Получение предсказаний модели\n",
    "1. Расчет функции потерь\n",
    "1. Записывание значения лосса\n",
    "\n",
    "Также с помощью контекста ```with torch.no_grad():``` можно явно указать торчу не сохранять необходимые параметры для расчета градиентов. Обязательно для режима предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "\n",
    "# это переводит модель в режим предсказания\n",
    "# то есть фиксируются статистики батч норма, дропаут не выкидывает фичи\n",
    "model.eval()\n",
    "\n",
    "# заметьте, что мы поменяли наш лоадер на валидационный\n",
    "for x, y in valid_loader:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # получение предсказаний модели\n",
    "        # расчет лосса\n",
    "        ...\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведите обучение несколько эпох\n",
    "Одна эпоха - это один проход по датасету.  \n",
    "Шаги:\n",
    "- Поменяйте что-нибудь в модели, добавить дропаут и тд\n",
    "- Остановите обучение с помощью early stopping\n",
    "- Добавьте расчет метрик во время обучения и предсказания (например, micro F1). Чтобы это сделать вы можете, например, сохранять предсказания модели\n",
    "- После обучения нарисуйте как по мере обучения меняется функция потерь на тренировочном и валидационном датасете, как меняется метрики\n",
    "- Опционально: постройте confusion matrix\n",
    "\n",
    "Подсказки:\n",
    "- Чтобы корректно сохранять предсказания нужно переменную отсоединить от графа, то есть сделать ```x.detach()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_epoch in range(2):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важные и не очень интуитивные моменты про LSTM и CNN в торче"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию LSTM принимает данные с такой размерностью:\n",
    "```python\n",
    "(seq_len, batch, input_size)\n",
    "```\n",
    "Сделано это с целью оптимизации на более низком уровне.  \n",
    "Мы оперируем такими объектами:\n",
    "```python\n",
    "(batch, seq_len, input_size)\n",
    "```\n",
    "Чтобы LSTM у нас заработала правильно, мы можем либо передать параметр ```batch_first=True``` во время инициализации слоя,\n",
    "либо транспонировать (поменять) первую и вторую размерность у нашего x перед подачей в слой.  \n",
    "[Подробнее про LSTM](https://pytorch.org/docs/stable/nn.html#lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 128 - размер батча\n",
    "- 64 - длина последовательности (количество слов)\n",
    "- 1024 - эмбеддинг слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(128, 64, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# первый способ\n",
    "lstm = torch.nn.LSTM(1024, 512, batch_first=True)\n",
    "\n",
    "pred, mem = lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# второй способ\n",
    "lstm = torch.nn.LSTM(1024, 512)\n",
    "\n",
    "# меняем размерность batch и seq_len местами\n",
    "x_transposed = x.transpose(0, 1)\n",
    "pred_transposed, mem = lstm(x_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у нас все еще осталась размерность (seq_len, batch, input_size)\n",
    "pred_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просто транспонируем еще раз\n",
    "pred = pred_transposed.transpose(0, 1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d & MaxPool1d\n",
    "Примерно такая же ситуация происходит со сверточными слоями и пулингами.  \n",
    "1d реализация как раз для текстов, в ней матрица-фильтр ходит только по одной размерности.  \n",
    "[Подробнее про CNN](https://pytorch.org/docs/stable/nn.html#conv1d)  \n",
    "[Подробнее про пулинг](https://pytorch.org/docs/stable/nn.html#maxpool1d)  \n",
    "Ожидается такая размерность:\n",
    "```python\n",
    "(batch, input_size, seq_len)\n",
    "```\n",
    "Мы все еще хоти подавать такую размерность:\n",
    "```python\n",
    "(batch, seq_len, input_size)\n",
    "```\n",
    "В случае со свертками и пулингами у нас есть вариант только транспонировать x перед подачей и транспонировать полученный результат. Обратите внимание, что транспонируем мы первую и вторую размерность (индексация с нуля)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels - размер входных эмбеддингов\n",
    "# out_channels - количество/какой размер эмбеддингов мы хотим получить\n",
    "# kernel_size - размер окна/н-граммы\n",
    "cnn = torch.nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выпадет ошибка, посмотрите какая\n",
    "# pred = cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transposed = x.transpose(1, 2)\n",
    "x_transposed.shape\n",
    "# перевели в (batch, input_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_transposed = cnn(x_transposed)\n",
    "pred_transposed.shape\n",
    "# осталась разрмерность (batch, output_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переведем обратно в (batch, seq_len, input_size)\n",
    "pred = pred_transposed.transpose(1, 2)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще важный момент про LSTM\n",
    "\n",
    "The input can also be a packed variable length sequence. See [torch.nn.utils.rnn.pack_padded_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) or [torch.nn.utils.rnn.pack_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) for details.\n",
    "\n",
    "Это внутренняя конструкция торча, которая позволяет не читать токен ```PAD```, но все еще работать с батчами. То есть внутри батча мы можем передать лстмке, что у нас данные переменной длины. Не забудьте что на выход отдается [torch.nn.utils.rnn.PackedSequence](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "\n",
    "1. Сделать класс нейронки, вписать необходимые операции, архитектура ниже\n",
    "1. Написать обучалку (обобщить то, что было выше)\n",
    "1. Добавить логирование\n",
    "    1. Сохранять лосс на каждой итерции обучения __0.25 балла__\n",
    "    1. Каждую эпоху сохранять лосс трейна и тест __0.25 балла__\n",
    "    1. Каждую эпоху рассчитывать метрики __0.25 балла__\n",
    "    1. Добавить прогресс бар, в котором показывается усредненный лосс последних 500-та итераций __0.25 балла__\n",
    "1. Добавить early stopping __0.5 балла__\n",
    "1. Нарисовать графики лосса, метрик, конфьюжин матрицу __0.5 балла__\n",
    "\n",
    "\n",
    "### Архитектура (что можно попробовать)\n",
    "1. Предобученные эмбеддинги. Почитайте [здесь](https://pytorch.org/docs/stable/nn.html#embedding) (from_pretrained) как вставить свои эмбеддинги, выше мы читали матрицу эмбеддингов. __0 баллов__\n",
    "1. Дообучить эмбеддинги отдельно от сети. __2 балла__\n",
    "1. Дообучить эмбеддинги вместе с сетью и с другим learning rate (указывается в оптимизаторе). __2 балла__\n",
    "1. Bidirectional LSTM. __1 балл__\n",
    "1. Несколько параллельных CNN с разными размерами окна и mean/max over time пулингами к ним и дальнейшей конкатенацией. __2 балла__\n",
    "1. Несколько последовательных CNN. __1 балла__\n",
    "1. Разные окна и residual к предыдущему пункту. __2 балла__\n",
    "1. Предыдущий пункт сделан без ошибок (замаскированы свертки паддингов). __2 балла__\n",
    "1. Написать правильный правильный mean/max пулинг, который не учитывает паддинги, точнее их маскирует. __2 балла__\n",
    "1. Добавить [torch.nn.utils.rnn.pack_padded_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) и [torch.nn.utils.rnn.pack_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) для LSTM. Инфа [здесь](#Еще-важный-момент-про-LSTM) __2 балла__\n",
    "1. Добавить spatial дропаут для входа LSTM (не просто стандартный пункт при инициализации LSTM) __1 балл__\n",
    "1. Добавить BatchNorm/LayerNorm/Dropout/Residual/etc __1 балл__\n",
    "1. Добавить шедуллер __1 балл__\n",
    "1. Обучать на GPU __2 балла__\n",
    "1. Сделать transfer learning с собственно обученной языковой модели, обученной на любых данных, например, unlabeled. __7 баллов__\n",
    "1. your madness\n",
    "\n",
    "## 10 баллов максимум\n",
    "\n",
    "# По итогам напишите результаты экспериментов\n",
    "# Что получилось, а что нет\n",
    "# Почему, выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
