{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом туториале будем обучать word2vec на своих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что делать?\n",
    "Где есть пометка # CODE писать код\n",
    "\n",
    "[Полезный туториал](http://jalammar.github.io/illustrated-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "Они уже обработанные и токенизированные. Процесс можно посмотреть в тетрадке `word2vec, part1. processing corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed_corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7528"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "идти домой свидетель жестокий драка двое один честно ребята\n",
      "самый красивый девочка ужас UNK длинный UNK спрашивать UNK\n",
      "злоумышленник убитый\n",
      "результат некоторый человек борт терять сознание другой страдать UNK атака\n",
      "UNK 2 видеть заниматься это дело UNK который изумительный UNK мешать UNK самый UNK линия\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:5]:\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'идти домой свидетель жестокий драка двое один честно ребята'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализуйте разделение предложения на примеры методом CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_split(tokens, window, pad_token='PAD'):\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    # CODE\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = cbow_split(sample_text, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in splits:\n",
    "    print('Левый контекст:', sample[0])\n",
    "    print('Центральное слово:', sample[1])\n",
    "    print('Правый контекст:', sample[2], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[(['PAD', 'PAD'], 'идти', ['домой', 'свидетель']),\n",
    " (['PAD', 'идти'], 'домой', ['свидетель', 'жестокий']),\n",
    " (['идти', 'домой'], 'свидетель', ['жестокий', 'драка']),\n",
    " (['домой', 'свидетель'], 'жестокий', ['драка', 'двое']),\n",
    " (['свидетель', 'жестокий'], 'драка', ['двое', 'один']),\n",
    " (['жестокий', 'драка'], 'двое', ['один', 'честно']),\n",
    " (['драка', 'двое'], 'один', ['честно', 'ребята']),\n",
    " (['двое', 'один'], 'честно', ['ребята', 'PAD']),\n",
    " (['один', 'честно'], 'ребята', ['PAD', 'PAD'])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_split(sample_text, window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[(['PAD', 'PAD', 'PAD'], 'идти', ['домой', 'свидетель', 'жестокий']),\n",
    " (['PAD', 'PAD', 'идти'], 'домой', ['свидетель', 'жестокий', 'драка']),\n",
    " (['PAD', 'идти', 'домой'], 'свидетель', ['жестокий', 'драка', 'двое']),\n",
    " (['идти', 'домой', 'свидетель'], 'жестокий', ['драка', 'двое', 'один']),\n",
    " (['домой', 'свидетель', 'жестокий'], 'драка', ['двое', 'один', 'честно']),\n",
    " (['свидетель', 'жестокий', 'драка'], 'двое', ['один', 'честно', 'ребята']),\n",
    " (['жестокий', 'драка', 'двое'], 'один', ['честно', 'ребята', 'PAD']),\n",
    " (['драка', 'двое', 'один'], 'честно', ['ребята', 'PAD', 'PAD']),\n",
    " (['двое', 'один', 'честно'], 'ребята', ['PAD', 'PAD', 'PAD'])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'идти домой свидетель жестокий драка двое один честно ребята'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализуйте разделение предложения на примеры методом Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_split(tokens, window):\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    # CODE\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = skipgram_split(sample_text, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in splits:\n",
    "    print('Контекст:', sample[0])\n",
    "    print('Центральное слово:', sample[1], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_split(sample_text, window=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[('домой', 'идти'),\n",
    " ('свидетель', 'идти'),\n",
    " ('идти', 'домой'),\n",
    " ('свидетель', 'домой'),\n",
    " ('жестокий', 'домой'),\n",
    " ('идти', 'свидетель'),\n",
    " ('домой', 'свидетель'),\n",
    " ('жестокий', 'свидетель'),\n",
    " ('драка', 'свидетель'),\n",
    " ('домой', 'жестокий'),\n",
    " ('свидетель', 'жестокий'),\n",
    " ('драка', 'жестокий'),\n",
    " ('двое', 'жестокий'),\n",
    " ('свидетель', 'драка'),\n",
    " ('жестокий', 'драка'),\n",
    " ('двое', 'драка'),\n",
    " ('один', 'драка'),\n",
    " ('жестокий', 'двое'),\n",
    " ('драка', 'двое'),\n",
    " ('один', 'двое'),\n",
    " ('честно', 'двое'),\n",
    " ('драка', 'один'),\n",
    " ('двое', 'один'),\n",
    " ('честно', 'один'),\n",
    " ('ребята', 'один'),\n",
    " ('двое', 'честно'),\n",
    " ('один', 'честно'),\n",
    " ('ребята', 'честно'),\n",
    " ('один', 'ребята'),\n",
    " ('честно', 'ребята')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_split(sample_text, window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected\n",
    "\n",
    "```python\n",
    "[('домой', 'идти'),\n",
    " ('свидетель', 'идти'),\n",
    " ('жестокий', 'идти'),\n",
    " ('идти', 'домой'),\n",
    " ('свидетель', 'домой'),\n",
    " ('жестокий', 'домой'),\n",
    " ('драка', 'домой'),\n",
    " ('идти', 'свидетель'),\n",
    " ('домой', 'свидетель'),\n",
    " ('жестокий', 'свидетель'),\n",
    " ('драка', 'свидетель'),\n",
    " ('двое', 'свидетель'),\n",
    " ('идти', 'жестокий'),\n",
    " ('домой', 'жестокий'),\n",
    " ('свидетель', 'жестокий'),\n",
    " ('драка', 'жестокий'),\n",
    " ('двое', 'жестокий'),\n",
    " ('один', 'жестокий'),\n",
    " ('домой', 'драка'),\n",
    " ('свидетель', 'драка'),\n",
    " ('жестокий', 'драка'),\n",
    " ('двое', 'драка'),\n",
    " ('один', 'драка'),\n",
    " ('честно', 'драка'),\n",
    " ('свидетель', 'двое'),\n",
    " ('жестокий', 'двое'),\n",
    " ('драка', 'двое'),\n",
    " ('один', 'двое'),\n",
    " ('честно', 'двое'),\n",
    " ('ребята', 'двое'),\n",
    " ('жестокий', 'один'),\n",
    " ('драка', 'один'),\n",
    " ('двое', 'один'),\n",
    " ('честно', 'один'),\n",
    " ('ребята', 'один'),\n",
    " ('драка', 'честно'),\n",
    " ('двое', 'честно'),\n",
    " ('один', 'честно'),\n",
    " ('ребята', 'честно'),\n",
    " ('двое', 'ребята'),\n",
    " ('один', 'ребята'),\n",
    " ('честно', 'ребята')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "\n",
    "for text in corpus:\n",
    "    for token in text:\n",
    "        if token not in word2index:\n",
    "            word2index[token] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2579"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[281, 1314, 13]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word2index[tok] if tok in word2index else word2index['UNK'] for tok in 'мама мыть рама'.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Для знакомства с Dataset вернитесь к туториалу `Word vectors.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 word2index,\n",
    "                 window=2,\n",
    "                 unk_token='UNK',\n",
    "                 pad_token='PAD',\n",
    "                 collect_verbose=True):\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.word2index = word2index\n",
    "        self.index2word = {value: key for key, value in self.word2index.items()}\n",
    "        self.window = window\n",
    "\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = self.word2index[self.unk_token]\n",
    "\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = len(self.word2index)\n",
    "\n",
    "        self.collect_verbose = collect_verbose\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        self.collect_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _split_function(self, tokenized_text):\n",
    "\n",
    "        splits = []\n",
    "\n",
    "        for n in range(len(tokenized_text)):\n",
    "            left_context = tokenized_text[np.maximum(n - self.window, 0):n]\n",
    "            left_context = ([self.pad_index] * (self.window - len(left_context))) + left_context\n",
    "\n",
    "            central_word = tokenized_text[n]\n",
    "\n",
    "            right_context = tokenized_text[n + 1:n + self.window + 1]\n",
    "            right_context = right_context + ([self.pad_index] * (self.window - len(right_context)))\n",
    "\n",
    "            splits.append((left_context + right_context, central_word))\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def indexing(self, tokenized_text):\n",
    "\n",
    "        return [self.word2index[token] if token in self.word2index else self.unk_index for token in tokenized_text]\n",
    "\n",
    "    def collect_data(self):\n",
    "\n",
    "        corpus = tqdm(self.corpus, disable=not self.collect_verbose)\n",
    "\n",
    "        for tokenized_text in corpus:\n",
    "            indexed_text = self.indexing(tokenized_text)\n",
    "            cbow_examples = self._split_function(indexed_text)\n",
    "\n",
    "            self.data.extend(cbow_examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        context, central_word = self.data[idx]\n",
    "\n",
    "        context = torch.Tensor(context).long()\n",
    "\n",
    "        return context, central_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мы будем учить модель Skipgram\n",
    "Реализуйте читалку данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 word2index,\n",
    "                 window=2,\n",
    "                 unk_token='UNK',\n",
    "                 collect_verbose=True):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def _split_function(self, tokenized_text):\n",
    "        \n",
    "        # CODE\n",
    "        \n",
    "        # вставить здесь функцию, которые вы писали раньше\n",
    "\n",
    "        splits = []\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def indexing(self, tokenized_text):\n",
    "\n",
    "        return [self.word2index[token] if token in self.word2index else self.unk_index for token in tokenized_text]\n",
    "\n",
    "    def collect_data(self):\n",
    "\n",
    "        corpus = tqdm(self.corpus, disable=not self.collect_verbose)\n",
    "\n",
    "        for tokenized_text in corpus:\n",
    "            indexed_text = self.indexing(tokenized_text)\n",
    "            skipgram_examples = self._split_function(indexed_text)\n",
    "\n",
    "            self.data.extend(skipgram_examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # CODE\n",
    "\n",
    "        return context, central_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 7528/7528 [00:00<00:00, 21374.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = SkipgramDataset(corpus, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  13, 2041,  495,  175,   13])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1104,   13,   13,   30, 1725])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]), torch.Size([128]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, pad_index):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if pad_index > 0:\n",
    "            vocab_size += 1\n",
    "        \n",
    "        self.in_embedding = torch.nn.Embedding(num_embeddings=vocab_size, \n",
    "                                               embedding_dim=embedding_dim,\n",
    "                                               padding_idx=pad_index)\n",
    "        \n",
    "        self.out_embedding = torch.nn.Linear(in_features=embedding_dim,\n",
    "                                             out_features=vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.in_embedding(x).sum(dim=-2)\n",
    "        x = self.out_embedding(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мы будем учить модель Skipgram\n",
    "Реализуйте ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "class SkipGram(torch.nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размерность эмбеддинга\n",
    "# маленькая, чтобы мы могли недолго поучить ворд2век и увидеть результаты\n",
    "EMBEDDING_DIM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size=len(word2index), embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "\n",
    "# aka loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишите обучалку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "losses = []\n",
    "\n",
    "for n_epoch in range(epochs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        progress_bar = tqdm(total=len(dataset_loader.dataset), desc='Epoch {}'.format(n_epoch + 1))\n",
    "\n",
    "        for x, y in dataset_loader:\n",
    "\n",
    "            # CODE\n",
    "\n",
    "            loss = # CODE\n",
    "            \n",
    "            # CODE\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix(loss=np.mean(losses[-100:]))\n",
    "\n",
    "            progress_bar.update(x.shape[0])\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        progress_bar.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Лайфхак__\n",
    "\n",
    "Если вы нажали `KeyboardInterrupt` и после этого tqdm начал печатать вывод каждый раз с новой строчки, сделайте так:\n",
    "\n",
    "```\n",
    "for instance in list(tqdm._instances):\n",
    "    tqdm._decr_instances(instance)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('SkipGram Training Process')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка, что хоть что-то выучилось\n",
    "assert np.mean(losses[-1000:]) < 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model.in_embedding.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(embedding_matrix, token2id, word1, word2):\n",
    "    \n",
    "    i1 = token2id[word1]\n",
    "    i2 = token2id[word2]\n",
    "    \n",
    "    v1, v2 = embedding_matrix[i1], embedding_matrix[i2]\n",
    "    \n",
    "    v1_n = v1.div(v1.norm(keepdim=True))\n",
    "    v2_n = v2.div(v2.norm(keepdim=True))\n",
    "    \n",
    "    similarity = torch.dot(v1_n, v2_n).item()\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Косинусная близость\n",
    "От -1 до 1, где -1 - вектора абсолютно разные, 1 - идентичные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'день', 'добрый')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'собака', 'кошка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(embedding_matrix, word2index, 'сотрудник', 'сотрудница')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(list(word2index.keys()))\n",
    "sim = cos_sim(embedding_matrix, word2index, 'день', random_word)\n",
    "'Косинусная близость слова \"день\" к случайному выбраному слову \"{}\" равна {:.3f}'.format(random_word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "\n",
    "for text in corpus:\n",
    "    for token in text:\n",
    "        if token in freq:\n",
    "            freq[token] += 1\n",
    "        else:\n",
    "            freq[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_freq = [(k, freq[k]) for k in sorted(freq, key=freq.get, reverse=True)]\n",
    "top_sorted_freq = sorted_freq[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация\n",
    "\n",
    "TSNE – метод снижения размерности. Применив его, мы получим вместо 20 координат для каждого эмбеддинга две, и сможем нарисовать то что у нас получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, init='pca', random_state=42, verbose=2)\n",
    "reduced = tsne.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = [a for a,_ in top_sorted_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = [word2index[word] for word in top_words]\n",
    "x_coords = [coords[0] for coords in reduced[inds]]\n",
    "y_coords = [coords[1] for coords in reduced[inds]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, word) in zip(x_coords, y_coords, top_words):\n",
    "    plt.scatter(x, y, marker='.', color='blue')\n",
    "    plt.text(x+0.01, y+0.01, word, fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n",
    "В обучении word2vec на последнем шаге происходит поиск слова в словаре по полученному эмбеддингу, и эта операция очень затратна по времени. Давайте мы не будем предсказывать слово `мыла` по слову `мама` (как делали только что), а вместо этого будем подавать на вход оба слова `мама` и `мыла` и просить модель предсказать вероятность того, что эти слова встретятся в одном контексте – это задача бинарной классификации.\n",
    "\n",
    "Тогда нам нужен соответствующий датасет. У нас уже есть пары слов, на которых мы учили модель Skipgram. Возьмём эти пары слов и проставим им класс 1 – эти слова точно встречаются в одном контексте. Теперь нам нужен класс 0 – слова, которые не встречаются в одном контексте. Для этого возьмём рандомные слова из нашего словаря, тогда вероятность появления какого-то слова равна количеству вхождений этого слова в корпус делённое на общее количество слов в корпусе: $$P(w_i) = \\frac{f(w_i)}{\\sum{_{j=0}^{n} f(w_j)}}$$ \n",
    "В оригинальной статье предлагается улучшение над этой формулой: возведём частоты в степень 3/4 и, таким образом, увеличим вероятность появления каких-то редких слов и снизим вероятность появления очень частотных слов (3/4 – это просто эвристика, которая хорошо сработала у авторов статьи, это никак не выводится математически, а просто вот так получилось и работает): $$P(w_i) = \\frac{(f(w_i))^\\frac{3}{4}}{\\sum{_{j=0}^{n} (f(w_j))^\\frac{3}{4}}}$$\n",
    "\n",
    "Но для простоты и в обучающих целях давайте всё-таки возьмём рандомные слова. Также важный вопрос, сколько негативных примеров брать на каждую пару примеров из корпуса. Давайте для простоты брать по одной, у нас получится полностью сбалансированный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 word2index,\n",
    "                 window=2,\n",
    "                 unk_token='UNK',\n",
    "                 collect_verbose=True):\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.word2index = word2index\n",
    "        self.index2word = {value: key for key, value in self.word2index.items()}\n",
    "        self.window = window\n",
    "\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = self.word2index[self.unk_token]\n",
    "\n",
    "        self.collect_verbose = collect_verbose\n",
    "\n",
    "        self.x_data = []\n",
    "        self.y_data = []   # здесь будем хранить классы – 0 или 1\n",
    "\n",
    "        self.collect_data()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def _split_function(self, tokens):\n",
    "        \n",
    "        splits = []\n",
    "        \n",
    "        for word in tokens:\n",
    "            index_central = tokens.index(word)\n",
    "            splits.extend([(tokens[i], word) for i in range(index_central - self.window, index_central) \\\n",
    "                           if i >= 0]) # left\n",
    "            splits.extend([(tokens[i], word) for i in range(index_central + 1, index_central + 1 + self.window) \\\n",
    "                       if i < len(tokens)]) # right\n",
    "\n",
    "        return splits\n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "        return [self.word2index[token] if token in self.word2index else self.unk_index for token in tokenized_text]\n",
    "\n",
    "    def collect_data(self):\n",
    "        corpus = tqdm(self.corpus, disable=not self.collect_verbose)\n",
    "\n",
    "        for tokenized_text in corpus:\n",
    "            negative_samples = []\n",
    "            \n",
    "            indexed_text = self.indexing(tokenized_text)\n",
    "            skipgram_examples = self._split_function(indexed_text)\n",
    "            \n",
    "            self.x_data.extend(skipgram_examples)\n",
    "            self.y_data.extend([1] * len(skipgram_examples))\n",
    "            \n",
    "            for w1, w2 in skipgram_examples:\n",
    "                negative_word = random.choice(range(len(self.word2index)))  # выбираем рандомное слово\n",
    "                \n",
    "                if negative_word != w2:\n",
    "                    negative_samples.append((w1, negative_word))\n",
    "                    \n",
    "            self.x_data.extend(negative_samples)\n",
    "            self.y_data.extend([0] * len(skipgram_examples))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, central_word = self.x_data[idx]\n",
    "        label = self.y_data[idx]\n",
    "        return (context, central_word), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7528/7528 [00:01<00:00, 6048.54it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_data = SkipgramNegDataset(corpus, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409704"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_data) # длина датасета увеличилась ровно в два раза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dataset_loader = DataLoader(neg_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для решения задачи классификации мы делаем очень просто: берём эмбеддинги первого и второго слова, перемножаем их и получаем какое-то число. Это число мы пропустим через сигмоиду и получим вероятность того, что эти слова встречаются в одном контексте. Посчитаем лосс относительно ground truth и обновим веса.\n",
    "\n",
    "(Однако помним, что у нас не одна пара слов, а целый датасет, поэтому перемножаем не одинокие эмбеддинги, а матрицы. Матрицы инициализируем рандомно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampling(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, pad_index=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embedding = torch.nn.Embedding(num_embeddings=vocab_size, \n",
    "                                                 embedding_dim=embedding_dim,\n",
    "                                                 padding_idx=pad_index)\n",
    "        \n",
    "        self.context_embedding = torch.nn.Embedding(num_embeddings=vocab_size, \n",
    "                                                    embedding_dim=embedding_dim,\n",
    "                                                    padding_idx=pad_index)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        x1 = self.word_embedding(x1)\n",
    "        x2 = self.context_embedding(x2)\n",
    "        dot_product = x1.matmul(x2.T).mean(axis=1)\n",
    "        return self.sigmoid(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NegativeSampling(vocab_size=len(word2index), embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████| 409704/409704 [00:30<00:00, 13394.98it/s, loss=0.726]\n",
      "Epoch 2: 100%|████████████| 409704/409704 [00:28<00:00, 14141.80it/s, loss=0.72]\n",
      "Epoch 3: 100%|███████████| 409704/409704 [00:29<00:00, 13788.67it/s, loss=0.717]\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "losses = []\n",
    "\n",
    "for n_epoch in range(epochs):\n",
    "\n",
    "    try:\n",
    "\n",
    "        progress_bar = tqdm(total=len(neg_dataset_loader.dataset), desc=f'Epoch {n_epoch + 1}')\n",
    "\n",
    "        for (x1, x2), y in dataset_loader:\n",
    "            \n",
    "            y = y.float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_out = model(x1, x2)\n",
    "            loss = criterion(x_out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix(loss=np.mean(losses[-100:]))\n",
    "\n",
    "            progress_bar.update(x1.shape[0])\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        progress_bar.close()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
