{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_classification(n_samples=250000, n_features=100, n_classes=4, n_clusters_per_class=5, n_informative=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(multi_class='auto', solver='lbfgs', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='auto', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.51      0.48     50025\n",
      "           1       0.42      0.31      0.35     50112\n",
      "           2       0.46      0.47      0.46     49875\n",
      "           3       0.47      0.55      0.51     49988\n",
      "\n",
      "   micro avg       0.46      0.46      0.46    200000\n",
      "   macro avg       0.45      0.46      0.45    200000\n",
      "weighted avg       0.45      0.46      0.45    200000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.51      0.48     12500\n",
      "           1       0.41      0.31      0.35     12368\n",
      "           2       0.46      0.47      0.47     12583\n",
      "           3       0.48      0.55      0.51     12549\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     50000\n",
      "   macro avg       0.45      0.46      0.45     50000\n",
      "weighted avg       0.45      0.46      0.45     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "print(classification_report(y_train, logreg.predict(x_train)))\n",
    "print('Test')\n",
    "print(classification_report(y_test, logreg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50),\n",
    "                    early_stopping=True,\n",
    "                    max_iter=15, \n",
    "                    verbose=True,\n",
    "                    validation_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04107034\n",
      "Validation score: 0.632200\n",
      "Iteration 2, loss = 0.79787499\n",
      "Validation score: 0.696850\n",
      "Iteration 3, loss = 0.69647365\n",
      "Validation score: 0.728125\n",
      "Iteration 4, loss = 0.63911510\n",
      "Validation score: 0.745100\n",
      "Iteration 5, loss = 0.60141722\n",
      "Validation score: 0.756225\n",
      "Iteration 6, loss = 0.57286984\n",
      "Validation score: 0.765100\n",
      "Iteration 7, loss = 0.55135924\n",
      "Validation score: 0.772525\n",
      "Iteration 8, loss = 0.53342468\n",
      "Validation score: 0.779325\n",
      "Iteration 9, loss = 0.51848544\n",
      "Validation score: 0.782900\n",
      "Iteration 10, loss = 0.50530846\n",
      "Validation score: 0.789275\n",
      "Iteration 11, loss = 0.49529823\n",
      "Validation score: 0.792850\n",
      "Iteration 12, loss = 0.48378324\n",
      "Validation score: 0.796350\n",
      "Iteration 13, loss = 0.47541960\n",
      "Validation score: 0.798100\n",
      "Iteration 14, loss = 0.46621147\n",
      "Validation score: 0.799450\n",
      "Iteration 15, loss = 0.45814216\n",
      "Validation score: 0.802425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 50), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=15, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.2, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MLP have 3 learnable layers\n",
      "And 15354 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum([mlp.coefs_[n].shape[0] * mlp.coefs_[n].shape[1] + mlp.intercepts_[n].shape[0] \n",
    "                    for n in range(len(mlp.coefs_))])\n",
    "\n",
    "print(f'Our MLP have {mlp.n_layers_ - 1} learnable layers') # 1 for input\n",
    "print(f'And {n_parameters} learnable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 with weight shape: (100, 100) and bias shape: (100,)\n",
      "Layer 2 with weight shape: (100, 50) and bias shape: (50,)\n",
      "Layer 3 with weight shape: (50, 4) and bias shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(mlp.coefs_)):\n",
    "    print(f'Layer {n+1} with weight shape: {mlp.coefs_[n].shape} and bias shape: {mlp.intercepts_[n].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our LogReg have 1 learnable layer\n",
      "And have 404 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'Our LogReg have 1 learnable layer')\n",
    "print(f'And have {logreg.coef_.shape[0] * logreg.coef_.shape[1] + logreg.intercept_.shape[0]} learnable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogReg looks much easier than MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85     50025\n",
      "           1       0.81      0.85      0.83     50112\n",
      "           2       0.84      0.83      0.83     49875\n",
      "           3       0.81      0.81      0.81     49988\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    200000\n",
      "   macro avg       0.83      0.83      0.83    200000\n",
      "weighted avg       0.83      0.83      0.83    200000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82     12500\n",
      "           1       0.77      0.82      0.80     12368\n",
      "           2       0.81      0.80      0.80     12583\n",
      "           3       0.78      0.78      0.78     12549\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     50000\n",
      "   macro avg       0.80      0.80      0.80     50000\n",
      "weighted avg       0.80      0.80      0.80     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "print(classification_report(y_train, mlp.predict(x_train)))\n",
    "print('Test')\n",
    "print(classification_report(y_test, mlp.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train much simpler MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(),\n",
    "                    early_stopping=True,\n",
    "                    max_iter=15, \n",
    "                    verbose=True,\n",
    "                    validation_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35639549\n",
      "Validation score: 0.449525\n",
      "Iteration 2, loss = 1.22501427\n",
      "Validation score: 0.446200\n",
      "Iteration 3, loss = 1.22500141\n",
      "Validation score: 0.449050\n",
      "Iteration 4, loss = 1.22499473\n",
      "Validation score: 0.447475\n",
      "Iteration 5, loss = 1.22508624\n",
      "Validation score: 0.447250\n",
      "Iteration 6, loss = 1.22497616\n",
      "Validation score: 0.448700\n",
      "Iteration 7, loss = 1.22508988\n",
      "Validation score: 0.448875\n",
      "Iteration 8, loss = 1.22493497\n",
      "Validation score: 0.449975\n",
      "Iteration 9, loss = 1.22513802\n",
      "Validation score: 0.449000\n",
      "Iteration 10, loss = 1.22498052\n",
      "Validation score: 0.449150\n",
      "Iteration 11, loss = 1.22494043\n",
      "Validation score: 0.449100\n",
      "Iteration 12, loss = 1.22499060\n",
      "Validation score: 0.445175\n",
      "Iteration 13, loss = 1.22498400\n",
      "Validation score: 0.447250\n",
      "Iteration 14, loss = 1.22498520\n",
      "Validation score: 0.446825\n",
      "Iteration 15, loss = 1.22514955\n",
      "Validation score: 0.446125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=15, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.2, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.51      0.48     50025\n",
      "           1       0.41      0.30      0.35     50112\n",
      "           2       0.45      0.48      0.47     49875\n",
      "           3       0.47      0.53      0.50     49988\n",
      "\n",
      "   micro avg       0.45      0.45      0.45    200000\n",
      "   macro avg       0.45      0.45      0.45    200000\n",
      "weighted avg       0.45      0.45      0.45    200000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.51      0.48     12500\n",
      "           1       0.41      0.30      0.34     12368\n",
      "           2       0.45      0.48      0.47     12583\n",
      "           3       0.47      0.53      0.50     12549\n",
      "\n",
      "   micro avg       0.45      0.45      0.45     50000\n",
      "   macro avg       0.45      0.45      0.45     50000\n",
      "weighted avg       0.45      0.45      0.45     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "print(classification_report(y_train, mlp.predict(x_train)))\n",
    "print('Test')\n",
    "print(classification_report(y_test, mlp.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our MLP have 1 learnable layers\n",
      "And 404 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum([mlp.coefs_[n].shape[0] * mlp.coefs_[n].shape[1] + mlp.intercepts_[n].shape[0] \n",
    "                    for n in range(len(mlp.coefs_))])\n",
    "\n",
    "print(f'Our MLP have {mlp.n_layers_ - 1} learnable layers') # 1 for input\n",
    "print(f'And {n_parameters} learnable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 with weight shape: (100, 4) and bias shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(mlp.coefs_)):\n",
    "    print(f'Layer {n+1} with weight shape: {mlp.coefs_[n].shape} and bias shape: {mlp.intercepts_[n].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our LogReg have 1 learnable layer\n",
      "And have 404 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'Our LogReg have 1 learnable layer')\n",
    "print(f'And have {logreg.coef_.shape[0] * logreg.coef_.shape[1] + logreg.intercept_.shape[0]} learnable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost same results compare to LogReg, because mlp with 1 learnable layer almost log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
